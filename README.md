# Sinhala-Bert
This model was created based on the initial configuration of the roBERTa model
This includes the following codes.

preprocess.py: Preprocess the dataset.
Zip_law.py: Checks whether the dataset behaves as Zipf's law
ngram.py: Gets the most frequents words in the dataset
ngram_perplexity.py: ngram perplexity can be calculated
Bert_tookenizor: Creates the Bert tokenizer for the dataset (using the libraries avvailable at huggingface)
Bert_model.py: Creates the Bert model based on RoBERTa configuration
