# Sinhala-Bert
This model was created based on the initial configuration of the roBERTa model
This includes the following codes.

preprocess.py: Preprocess the dataset.__
Zip_law.py: Checks whether the dataset behaves as Zipf's law.__
ngram.py: Gets the most frequents words in the dataset.__
ngram_perplexity.py: ngram perplexity can be calculated.__
Bert_tookenizor: Creates the Bert tokenizer for the dataset (using the libraries avvailable at huggingface).__
Bert_model.py: Creates the Bert model based on RoBERTa configuration.
