# Sinhala-Bert
This model was created based on the initial configuration of the roBERTa model
This includes the following codes.

preprocess.py: Preprocess the dataset.<br />
Zip_law.py: Checks whether the dataset behaves as Zipf's law.<br />
ngram.py: Gets the most frequents words in the dataset.<br />
ngram_perplexity.py: ngram perplexity can be calculated.<br />
Bert_tookenizor: Creates the Bert tokenizer for the dataset (using the libraries avvailable at huggingface).<br />
Bert_model.py: Creates the Bert model based on RoBERTa configuration.
